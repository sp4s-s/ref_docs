import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import numpy as np

class SimpleNN(nn.Module):
    def __init__(self, i, h, o):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(i, h)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(h, o)
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

class DeeperNN(nn.Module):
    def __init__(self, i, h1, h2, o):
        super(DeeperNN, self).__init__()
        self.fc1 = nn.Linear(i, h1)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(h1, h2)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(h2, o)
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        x = self.relu2(x)
        x = self.fc3(x)
        return x

np.random.seed(42)
X = np.random.rand(100, 10)
y = np.random.randint(0, 2, 100)
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.long)
X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

def train_and_validate(model, train_loader, val_loader, optimizer, criterion, epochs):
    best_val_accuracy = 0.0
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * batch_x.size(0)
        train_loss /= len(train_loader.dataset)
        model.eval()
        val_loss = 0.0
        correct = 0
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item() * batch_x.size(0)
                _, predicted = torch.max(outputs.data, 1)
                correct += (predicted == batch_y).sum().item()
        val_loss /= len(val_loader.dataset)
        val_accuracy = correct / len(val_loader.dataset)
        print(f"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            best_model_state = model.state_dict()
    return best_val_accuracy, best_model_state

input_size = 10
output_size = 2
criterion = nn.CrossEntropyLoss()
hidden_sizes_nn1 = [8, 16]
hidden_sizes_nn2 = [(16, 8), (32, 16)]
learning_rates = [0.01, 0.001]
epochs = 50
best_model = None
best_accuracy = 0.0
best_hyperparams = None

for nn_type in ['SimpleNN', 'DeeperNN']:
    if nn_type == 'SimpleNN':
        for hidden_size in hidden_sizes_nn1:
            for lr in learning_rates:
                model = SimpleNN(input_size, hidden_size, output_size)
                optimizer = optim.Adam(model.parameters(), lr=lr)
                print(f"Training SimpleNN with hidden_size={hidden_size}, lr={lr}")
                val_accuracy, model_state = train_and_validate(model, train_loader, val_loader, optimizer, criterion, epochs)
                if val_accuracy > best_accuracy:
                    best_accuracy = val_accuracy
                    best_model = model_state
                    best_hyperparams = {'model_type': 'SimpleNN', 'hidden_size': hidden_size, 'lr': lr}
    elif nn_type == 'DeeperNN':
        for hidden_size1, hidden_size2 in hidden_sizes_nn2:
            for lr in learning_rates:
                model = DeeperNN(input_size, hidden_size1, hidden_size2, output_size)
                optimizer = optim.Adam(model.parameters(), lr=lr)
                print(f"Training DeeperNN with hidden_size1={hidden_size1}, hidden_size2={hidden_size2}, lr={lr}")
                val_accuracy, model_state = train_and_validate(model, train_loader, val_loader, optimizer, criterion, epochs)
                if val_accuracy > best_accuracy:
                    best_accuracy = val_accuracy
                    best_model = model_state
                    best_hyperparams = {'model_type': 'DeeperNN', 'hidden_size1': hidden_size1, 'hidden_size2': hidden_size2, 'lr': lr}

print("\nBest Model:")
print(f"Model Type: {best_hyperparams['model_type']}")
if best_hyperparams['model_type'] == 'SimpleNN':
    print(f"Hidden Size: {best_hyperparams['hidden_size']}")
elif best_hyperparams['model_type'] == 'DeeperNN':
    print(f"Hidden Layer 1 Size: {best_hyperparams['hidden_size1']}, Hidden Layer 2 Size: {best_hyperparams['hidden_size2']}")
print(f"Learning Rate: {best_hyperparams['lr']}")
print(f"Best Validation Accuracy: {best_accuracy:.4f}")

if best_hyperparams['model_type'] == 'SimpleNN':
    final_model = SimpleNN(input_size, best_hyperparams['hidden_size'], output_size)
elif best_hyperparams['model_type'] == 'DeeperNN':
    final_model = DeeperNN(input_size, best_hyperparams['hidden_size1'],best_hyperparams['hidden_size2'], output_size)
final_model.load_state_dict(best_model)
